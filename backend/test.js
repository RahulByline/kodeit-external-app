import axios from 'axios';

async function testOllamaConnection() {
  try {
    console.log('ðŸ§ª Testing Ollama connection...');
    
    // Test if Ollama is running
    const response = await axios.get('http://localhost:11434/api/tags');
    console.log('âœ… Ollama is running!');
    console.log('ðŸ“‹ Available models:', response.data.models?.map(m => m.name) || 'No models found');
    
    // Test if Mistral model is available
    const models = response.data.models || [];
    const mistralAvailable = models.some(model => model.name.includes('mistral'));
    
    if (mistralAvailable) {
      console.log('âœ… Mistral model is available!');
    } else {
      console.log('âš ï¸  Mistral model not found. Please run: ollama pull mistral');
    }
    
  } catch (error) {
    console.error('âŒ Ollama connection failed:', error.message);
    console.log('ðŸ’¡ Make sure Ollama is running: ollama serve');
  }
}

async function testBackendEndpoint() {
  try {
    console.log('\nðŸ§ª Testing backend endpoint...');
    
    const response = await axios.post('http://localhost:5000/chat', {
      message: 'Hello, this is a test message!'
    });
    
    console.log('âœ… Backend is working!');
    console.log('ðŸ¤– AI Response:', response.data.response.substring(0, 100) + '...');
    console.log('ðŸ“… Timestamp:', response.data.timestamp);
    console.log('ðŸ·ï¸  Source:', response.data.source);
    
  } catch (error) {
    console.error('âŒ Backend test failed:', error.message);
    console.log('ðŸ’¡ Make sure the backend is running: npm run dev');
  }
}

// Run tests
console.log('ðŸš€ My AI Buddy - Connection Tests\n');
testOllamaConnection().then(() => {
  setTimeout(testBackendEndpoint, 1000);
}); 