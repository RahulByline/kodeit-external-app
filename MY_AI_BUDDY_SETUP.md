# My AI Buddy - Setup Guide

## ğŸš€ Overview

My AI Buddy is a professional AI chatbot integrated into your full-stack React application. It uses Ollama with the Mistral model for local AI processing.

## ğŸ“‹ Prerequisites

1. **Ollama installed and running** with Mistral model
2. **Node.js** (v16 or higher)
3. **Your React app** (already set up)

## ğŸ”§ Backend Setup

### 1. Install Backend Dependencies

```bash
cd backend
npm install
```

### 2. Start the Backend Server

```bash
# Development mode (with auto-restart)
npm run dev

# Production mode
npm start
```

The backend will run on `http://localhost:5000`

## ğŸ¨ Frontend Setup

### 1. The React component is already integrated

The `MyAIBuddy.tsx` component is already:
- âœ… Created in `src/components/MyAIBuddy.tsx`
- âœ… Integrated into `src/App.tsx`
- âœ… Uses your existing UI components (shadcn/ui)

### 2. Start the Frontend

```bash
# In your project root
npm run dev
```

The frontend will run on `http://localhost:3000`

## ğŸ¤– Ollama Setup

### 1. Install Ollama

Visit [ollama.ai](https://ollama.ai) and install Ollama for your platform.

### 2. Pull the Mistral Model

```bash
ollama pull mistral
```

### 3. Start Ollama

```bash
ollama serve
```

Ollama will run on `http://localhost:11434`

## ğŸ¯ How to Use

1. **Start all services:**
   - Ollama server (`ollama serve`)
   - Backend (`cd backend && npm run dev`)
   - Frontend (`npm run dev`)

2. **Open your app** at `http://localhost:3000`

3. **Look for the chat button** in the bottom-right corner

4. **Click the chat button** to open My AI Buddy

5. **Start chatting!** The chatbot will:
   - Maintain conversation context
   - Show loading animations
   - Display timestamps and source info
   - Support both English and Arabic

## ğŸ—ï¸ Architecture

```
Frontend (React) â†’ Backend (Express) â†’ Ollama (Mistral)
     â†“                    â†“                    â†“
localhost:3000    localhost:5000      localhost:11434
```

## ğŸ“ File Structure

```
backend/
â”œâ”€â”€ package.json          # Backend dependencies
â””â”€â”€ index.js             # Express server with /chat endpoint

src/
â”œâ”€â”€ components/
â”‚   â””â”€â”€ MyAIBuddy.tsx   # React chatbot component
â””â”€â”€ App.tsx              # Main app (already integrated)
```

## ğŸ”§ Features

### Backend Features
- âœ… POST `/chat` endpoint
- âœ… Conversation memory (last 20 messages)
- âœ… Error handling
- âœ… CORS enabled
- âœ… Structured JSON responses
- âœ… Health check endpoint

### Frontend Features
- âœ… Floating chat button
- âœ… Professional UI with shadcn/ui
- âœ… Real-time messaging
- âœ… Loading animations
- âœ… Auto-scroll to latest messages
- âœ… Responsive design
- âœ… Dark mode support
- âœ… Keyboard shortcuts (Enter to send)
- âœ… Error handling

## ğŸ› Troubleshooting

### Backend Issues
1. **Port 5000 already in use:**
   ```bash
   # Change port in backend/index.js
   const PORT = 5001; // or any available port
   ```

2. **Ollama connection failed:**
   - Ensure Ollama is running: `ollama serve`
   - Check if Mistral model is installed: `ollama list`

### Frontend Issues
1. **Chat button not appearing:**
   - Check browser console for errors
   - Ensure MyAIBuddy component is imported in App.tsx

2. **Messages not sending:**
   - Verify backend is running on correct port
   - Check network tab for API errors

## ğŸ¨ Customization

### Change Chatbot Position
Edit `MyAIBuddy.tsx`:
```tsx
// Change from bottom-right to bottom-left
className="fixed bottom-6 left-6 w-96 h-[600px] z-50"
```

### Change Chatbot Size
```tsx
// Make it wider
className="fixed bottom-6 right-6 w-[500px] h-[700px] z-50"
```

### Change AI Model
Edit `backend/index.js`:
```javascript
// Change from mistral to another model
model: 'llama2', // or any other model you have
```

## ğŸš€ Production Deployment

### Backend Deployment
1. Use PM2 or similar process manager
2. Set up environment variables
3. Use a proper database for conversation storage

### Frontend Deployment
1. Build the app: `npm run build`
2. Deploy to Vercel, Netlify, or your preferred platform

## ğŸ“ Support

If you encounter any issues:
1. Check the browser console for errors
2. Verify all services are running
3. Ensure Ollama has the Mistral model installed

---

**Happy chatting with My AI Buddy! ğŸ¤–âœ¨** 